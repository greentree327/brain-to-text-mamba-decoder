{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ecde7b7",
   "metadata": {},
   "source": [
    "# Brain-to-Text Inference Pipeline\n",
    "\n",
    "This notebook replicates the original `slightly_tidy_version_Mamba_GRU_LISA_Ensemble_with_TTA` notebook using modular code from `src/`.\n",
    "\n",
    "**What this does:**\n",
    "1. Load neural data (HDF5)\n",
    "2. Run Mamba + GRU models\n",
    "3. Ensemble predictions (LISA)\n",
    "4. Compute metrics (WER/CER)\n",
    "5. Show results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef309c7",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8571f657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Import from modular code\n",
    "from src.models import MambaDecoder, GRUDecoderBaseline\n",
    "from src.data_loader import BrainToTextDataset, create_data_loader\n",
    "from src.decoding import (\n",
    "    run_single_decoding_step,\n",
    "    ensemble_logit_averaging,\n",
    "    ensemble_majority_vote,\n",
    "    LISAEnsemble,\n",
    "    apply_test_time_augmentation\n",
    ")\n",
    "from src.utils import compute_wer, compute_cer, gauss_smooth, phoneme_ids_to_text\n",
    "\n",
    "# Check GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3108b43",
   "metadata": {},
   "source": [
    "## 1.5. Kaggle + Hugging Face Authentication & Downloads\n",
    "\n",
    "You can authenticate in two ways:\n",
    "\n",
    "1) Interactive login (recommended for first time)\n",
    "2) .env file with credentials\n",
    "\n",
    "**.env example:**\n",
    "```\n",
    "KAGGLE_USERNAME=your_username\n",
    "KAGGLE_KEY=your_api_key\n",
    "HUGGINGFACE_TOKEN=your_hf_token\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62311a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import kagglehub\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# Optional: load credentials from .env\n",
    "# Install once if needed: pip install python-dotenv\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "\n",
    "    load_dotenv()\n",
    "except Exception:\n",
    "    print(\"python-dotenv not available; skipping .env loading\")\n",
    "\n",
    "# If .env provides Kaggle credentials, set environment variables\n",
    "kaggle_user = os.getenv(\"KAGGLE_USERNAME\")\n",
    "kaggle_key = os.getenv(\"KAGGLE_KEY\")\n",
    "if kaggle_user and kaggle_key:\n",
    "    os.environ[\"KAGGLE_USERNAME\"] = kaggle_user\n",
    "    os.environ[\"KAGGLE_KEY\"] = kaggle_key\n",
    "    print(\"Kaggle credentials loaded from .env\")\n",
    "else:\n",
    "    print(\"No Kaggle credentials found; Kaggle will prompt interactively\")\n",
    "\n",
    "# Hugging Face login (token from .env if available)\n",
    "# If token is missing, this will open an interactive prompt\n",
    "hf_token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "notebook_login(token=hf_token)\n",
    "\n",
    "# Login to Kaggle\n",
    "kagglehub.login()\n",
    "\n",
    "# Download competition dataset\n",
    "competition_name = \"brain-to-text-25\"  # Update with actual competition name\n",
    "print(\"Downloading dataset from Kaggle...\")\n",
    "dataset_path = kagglehub.competition_download(competition_name)\n",
    "print(f\"Dataset downloaded to: {dataset_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390b149a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_sources import download_all_sources\n",
    "\n",
    "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
    "# THEN FEEL FREE TO DELETE THIS CELL.\n",
    "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
    "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
    "# NOTEBOOK.\n",
    "\n",
    "sources = download_all_sources()\n",
    "\n",
    "# Example: use main dataset path from sources\n",
    "# dataset_path = sources[\"brain_to_text_25_path\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6461691c",
   "metadata": {},
   "source": [
    "## 2. Load Data\n",
    "\n",
    "Data will be loaded from the Kaggle download path.\n",
    "**Optional:** Update paths below if using your own local files instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f913389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from Kaggle download (if available) or specify manual paths\n",
    "if 'sources' in locals() and \"brain_to_text_25_path\" in sources:\n",
    "    dataset_path = sources[\"brain_to_text_25_path\"]\n",
    "\n",
    "if 'dataset_path' in locals():\n",
    "    # Use Kaggle downloaded data\n",
    "    data_path = f\"{dataset_path}/neural_data.h5\"  # Update filename if different\n",
    "    metadata_path = f\"{dataset_path}/metadata.csv\"  # Update filename if different\n",
    "    print(f\"Using Kaggle dataset from: {dataset_path}\")\n",
    "else:\n",
    "    # Manual paths (update these if not using Kaggle download)\n",
    "    data_path = 'path/to/your/data.h5'\n",
    "    metadata_path = 'path/to/your/metadata.csv'\n",
    "    print(\"Using manual paths\")\n",
    "\n",
    "# Load dataset\n",
    "dataset = BrainToTextDataset(\n",
    "    data_path=data_path,\n",
    "    metadata_path=metadata_path\n",
    ")\n",
    "\n",
    "# Create DataLoader\n",
    "test_loader = create_data_loader(\n",
    "    dataset=dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=0  # Set to 0 on Windows to avoid multiprocessing issues\n",
    ")\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)} samples\")\n",
    "print(f\"Number of batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa99ccc4",
   "metadata": {},
   "source": [
    "## 3. Initialize Models\n",
    "\n",
    "Create Mamba and GRU decoders with your configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe60bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration (update based on your data)\n",
    "config = {\n",
    "    'neural_dim': 513,      # Neural feature dimension\n",
    "    'n_units': 256,         # Hidden units\n",
    "    'n_days': 5,            # Number of recording days\n",
    "    'n_classes': 40,        # Number of phoneme classes\n",
    "    'n_layers': 3,          # Number of Mamba layers\n",
    "    'drop_path': 0.1        # Stochastic depth rate\n",
    "}\n",
    "\n",
    "# Initialize Mamba model\n",
    "mamba_model = MambaDecoder(\n",
    "    neural_dim=config['neural_dim'],\n",
    "    n_units=config['n_units'],\n",
    "    n_days=config['n_days'],\n",
    "    n_classes=config['n_classes'],\n",
    "    n_layers=config['n_layers'],\n",
    "    drop_path=config['drop_path']\n",
    ").to(device)\n",
    "\n",
    "# Initialize GRU model\n",
    "gru_model = GRUDecoderBaseline(\n",
    "    neural_dim=config['neural_dim'],\n",
    "    hidden_dim=config['n_units'],\n",
    "    n_days=config['n_days'],\n",
    "    n_classes=config['n_classes'],\n",
    "    n_layers=config['n_layers']\n",
    ").to(device)\n",
    "\n",
    "print(f\"Mamba parameters: {sum(p.numel() for p in mamba_model.parameters()):,}\")\n",
    "print(f\"GRU parameters: {sum(p.numel() for p in gru_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8532bd2b",
   "metadata": {},
   "source": [
    "## 4. Load Pretrained Weights\n",
    "\n",
    "Models will be loaded from Kaggle download path (if available).\n",
    "**Optional:** Update paths below if using your own local model files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a904f5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from Kaggle download (if available) or specify manual paths\n",
    "if 'dataset_path' in locals():\n",
    "    # Use Kaggle downloaded models\n",
    "    mamba_checkpoint = f\"{dataset_path}/mamba_model.pth\"  # Update filename if different\n",
    "    gru_checkpoint = f\"{dataset_path}/gru_model.pth\"  # Update filename if different\n",
    "    print(f\"Loading models from Kaggle: {dataset_path}\")\n",
    "else:\n",
    "    # Manual paths (update these if not using Kaggle download)\n",
    "    mamba_checkpoint = 'path/to/mamba_model.pth'\n",
    "    gru_checkpoint = 'path/to/gru_model.pth'\n",
    "    print(\"Using manual model paths\")\n",
    "\n",
    "# Load weights\n",
    "mamba_model.load_state_dict(torch.load(mamba_checkpoint, map_location=device))\n",
    "gru_model.load_state_dict(torch.load(gru_checkpoint, map_location=device))\n",
    "\n",
    "# Set to evaluation mode\n",
    "mamba_model.eval()\n",
    "gru_model.eval()\n",
    "\n",
    "print(\"✓ Models loaded and ready for inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4ebcfd",
   "metadata": {},
   "source": [
    "## 5. Run Inference\n",
    "\n",
    "Process data through both models and collect predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2a7e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = []\n",
    "all_targets = []\n",
    "mamba_logits_list = []\n",
    "gru_logits_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (neural_data, day_idx, targets) in enumerate(test_loader):\n",
    "        # Move to device\n",
    "        neural_data = neural_data.to(device)\n",
    "        day_idx = day_idx.to(device)\n",
    "        \n",
    "        # Mamba inference\n",
    "        mamba_logits = run_single_decoding_step(\n",
    "            model=mamba_model,\n",
    "            neural_data=neural_data,\n",
    "            day_idx=day_idx,\n",
    "            apply_smoothing=True,\n",
    "            sigma=1.5\n",
    "        )\n",
    "        \n",
    "        # GRU inference\n",
    "        gru_logits = run_single_decoding_step(\n",
    "            model=gru_model,\n",
    "            neural_data=neural_data,\n",
    "            day_idx=day_idx,\n",
    "            apply_smoothing=True,\n",
    "            sigma=1.5\n",
    "        )\n",
    "        \n",
    "        # Store logits for ensemble\n",
    "        mamba_logits_list.append(mamba_logits.cpu().numpy())\n",
    "        gru_logits_list.append(gru_logits.cpu().numpy())\n",
    "        all_targets.append(targets.numpy())\n",
    "        \n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            print(f\"Processed {batch_idx + 1}/{len(test_loader)} batches\")\n",
    "\n",
    "print(\"\\n✓ Inference complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e769ce9e",
   "metadata": {},
   "source": [
    "## 6. Ensemble Predictions (LISA)\n",
    "\n",
    "Combine Mamba and GRU predictions using LISA ensemble method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bea5036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all batches\n",
    "mamba_logits_all = np.concatenate(mamba_logits_list, axis=0)\n",
    "gru_logits_all = np.concatenate(gru_logits_list, axis=0)\n",
    "targets_all = np.concatenate(all_targets, axis=0)\n",
    "\n",
    "# LISA Ensemble\n",
    "lisa = LISAEnsemble(strategy='weighted', weights=[0.6, 0.4])  # Mamba 60%, GRU 40%\n",
    "ensemble_logits = lisa.aggregate([mamba_logits_all, gru_logits_all])\n",
    "\n",
    "# Get predictions\n",
    "ensemble_preds = np.argmax(ensemble_logits, axis=-1)\n",
    "mamba_preds = np.argmax(mamba_logits_all, axis=-1)\n",
    "gru_preds = np.argmax(gru_logits_all, axis=-1)\n",
    "\n",
    "print(f\"Ensemble predictions shape: {ensemble_preds.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a8956b",
   "metadata": {},
   "source": [
    "## 7. Compute Metrics\n",
    "\n",
    "Calculate Word Error Rate (WER) and Character Error Rate (CER)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1397bfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert phoneme IDs to text\n",
    "def batch_phoneme_to_text(phoneme_ids):\n",
    "    \"\"\"Convert batch of phoneme IDs to text.\"\"\"\n",
    "    texts = []\n",
    "    for seq in phoneme_ids:\n",
    "        text = phoneme_ids_to_text(seq)\n",
    "        texts.append(text)\n",
    "    return texts\n",
    "\n",
    "# Convert predictions and targets to text\n",
    "ensemble_texts = batch_phoneme_to_text(ensemble_preds)\n",
    "mamba_texts = batch_phoneme_to_text(mamba_preds)\n",
    "gru_texts = batch_phoneme_to_text(gru_preds)\n",
    "target_texts = batch_phoneme_to_text(targets_all)\n",
    "\n",
    "# Compute metrics\n",
    "def compute_metrics(predictions, targets):\n",
    "    wers = [compute_wer(pred, tgt) for pred, tgt in zip(predictions, targets)]\n",
    "    cers = [compute_cer(pred, tgt) for pred, tgt in zip(predictions, targets)]\n",
    "    return np.mean(wers), np.mean(cers)\n",
    "\n",
    "ensemble_wer, ensemble_cer = compute_metrics(ensemble_texts, target_texts)\n",
    "mamba_wer, mamba_cer = compute_metrics(mamba_texts, target_texts)\n",
    "gru_wer, gru_cer = compute_metrics(gru_texts, target_texts)\n",
    "\n",
    "# Print results\n",
    "print(\"=\"*60)\n",
    "print(\"RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Mamba Model:     WER = {mamba_wer:.4f} ({mamba_wer*100:.2f}%)  |  CER = {mamba_cer:.4f}\")\n",
    "print(f\"GRU Model:       WER = {gru_wer:.4f} ({gru_wer*100:.2f}%)  |  CER = {gru_cer:.4f}\")\n",
    "print(f\"LISA Ensemble:   WER = {ensemble_wer:.4f} ({ensemble_wer*100:.2f}%)  |  CER = {ensemble_cer:.4f}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Improvement: {(mamba_wer - ensemble_wer) / mamba_wer * 100:.1f}% relative reduction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67d8b36",
   "metadata": {},
   "source": [
    "## 8. Show Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c096454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first 5 examples\n",
    "print(\"\\nSample Predictions:\\n\")\n",
    "for i in range(min(5, len(target_texts))):\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"  Target:    {target_texts[i]}\")\n",
    "    print(f\"  Mamba:     {mamba_texts[i]}\")\n",
    "    print(f\"  GRU:       {gru_texts[i]}\")\n",
    "    print(f\"  Ensemble:  {ensemble_texts[i]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb719fc6",
   "metadata": {},
   "source": [
    "## 9. Optional: Test-Time Augmentation (TTA)\n",
    "\n",
    "Apply TTA for additional performance boost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb522088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply TTA to first batch as example\n",
    "sample_batch = next(iter(test_loader))\n",
    "neural_data, day_idx, targets = sample_batch\n",
    "neural_data = neural_data.to(device)\n",
    "day_idx = day_idx.to(device)\n",
    "\n",
    "# Run TTA\n",
    "tta_logits = apply_test_time_augmentation(\n",
    "    model=mamba_model,\n",
    "    neural_data=neural_data,\n",
    "    day_idx=day_idx,\n",
    "    n_augmentations=5,\n",
    "    noise_std=0.01\n",
    ")\n",
    "\n",
    "tta_preds = np.argmax(tta_logits, axis=-1)\n",
    "tta_texts = batch_phoneme_to_text(tta_preds)\n",
    "target_texts_sample = batch_phoneme_to_text(targets.numpy())\n",
    "\n",
    "tta_wer, tta_cer = compute_metrics(tta_texts, target_texts_sample)\n",
    "print(f\"TTA Results: WER = {tta_wer:.4f} ({tta_wer*100:.2f}%)  |  CER = {tta_cer:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2087e11f",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. ✅ Loading neural data using modular `src.data_loader`\n",
    "2. ✅ Running Mamba and GRU models from `src.models`\n",
    "3. ✅ Ensemble predictions using LISA from `src.decoding`\n",
    "4. ✅ Computing metrics using `src.utils`\n",
    "5. ✅ Test-time augmentation for improved accuracy\n",
    "\n",
    "**Next steps:**\n",
    "- Fine-tune ensemble weights for your specific data\n",
    "- Experiment with different TTA strategies\n",
    "- Deploy as production inference pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
